{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Project 1 - Part 2: Train an Image Classifier from Scratch</font>\n",
    "As discussed in the previous notebook, the steps for training Neural Networks are:\n",
    "\n",
    "- Step 1 - Understand your problem\n",
    "- Step 2A - Get the data\n",
    "- Step 2B - Explore & Understand your data\n",
    "- Step 2C - Create a sample data from the dataset\n",
    "- Step 3 - Data Preparation\n",
    "- Step 4 - Train a simple model on sample data and check the pipeline before proceeding to train the full network\n",
    "- Step 5 - Train on Full Data\n",
    "- Step 6 - Improve your model\n",
    "\n",
    "You have already gone through Steps 1 through 4 in the previous notebook and you can use them in this notebook as well.\n",
    "\n",
    "In this notebook You will implement Steps 5 & 6 completely from scratch. You should design a model which achieves 85% validation accuracy on the given dataset.\n",
    "\n",
    "There are **70 points** for this notebook. <font style=\"color:red\">The sections which carry marks are written in Red for easy reference.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you have received Microsoft Azure Credits, you can use the GPUs for training the network (We will provide instructions on how to set it up in case you are not able to figure it out yourself). After you have completed the assignment, upload and submit it on the portal so that we can check and provide feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style=\"color:red\">You will use the full data to train the network. You need to achieve 85% accuracy for validation data to successfully complete this assignment.</font>\n",
    "\n",
    "<font style=\"color:red\">As mentioned earlier, you should build your own model and not use any pre-trained models/weights.</font>\n",
    "\n",
    "After you have completed the assignment, <font style=\"color:red\">upload and submit the notebook and the models folder on the portal</font> so that we can check and provide feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Step 1: Understand your problem </font>\n",
    "Already covered in previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Step 2: Get the data </font>\n",
    "Already covered in previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://www.dropbox.com/sh/n5nya3g3airlub6/AACi7vaUjdTA0t2j_iKWgp4Ra?dl=1\" -O data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:green\">Extract the data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Step 3. Data Preparation</font>\n",
    "Already covered in previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">3.1. Import Libraries </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from typing import Iterable\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:green\">3.2.1. Compulsary preprocessing transforms</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocess_transforms():\n",
    "    \n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "    \n",
    "    return preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:green\">3.2.2. Common Image Transforms</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_common_transforms(mean=(0.4611, 0.4359, 0.3905), std=(0.2193, 0.2150, 0.2109)):\n",
    "    preprocess = image_preprocess_transforms()\n",
    "    \n",
    "    common_transforms = transforms.Compose([\n",
    "        preprocess,\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    \n",
    "    return common_transforms\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:green\">3.2.3. Mean and STD</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std(data_root, num_workers=4):\n",
    "    \n",
    "    transform = image_preprocess_transforms()\n",
    "    \n",
    "    loader = data_loader(data_root, transform)\n",
    "\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    \n",
    "    for images, _ in loader:\n",
    "        batch_samples = images.size(0) # batch size (the last batch can have smaller size!)\n",
    "        images = images.view(batch_samples, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "\n",
    "    mean /= len(loader.dataset)\n",
    "    std /= len(loader.dataset)\n",
    "    \n",
    "    print('mean: {}, std: {}'.format(mean, std))\n",
    "    \n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">3.3. Data Loaders </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:green\">3.3.1. Data Loader for Full Data</font>\n",
    "Data loader for generating batches of data to be used by the training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(data_root, transform, batch_size=16, shuffle=False, num_workers=2):\n",
    "    dataset = datasets.ImageFolder(root=data_root, transform=transform)\n",
    "    \n",
    "    loader = torch.utils.data.DataLoader(dataset, \n",
    "                                         batch_size=batch_size,\n",
    "                                         num_workers=num_workers,\n",
    "                                         shuffle=shuffle)\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">3.4. Prepare Data</font>\n",
    "The main function which uses all the above functions to generate the train and test dataloaders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(batch_size, data_root, num_workers=4, data_augmentation=False):\n",
    "#     YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Step 4: Train your Model</font>\n",
    "It is time to create the training pipeline and train your model on the full data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:red\">4.1. Configurations [20 Points]</font>\n",
    "You need to make changes to the configurations. To achieve good results the parameters given in these configurations needs to be changed. You get 10 points for making the right choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:green\">4.1.1. System Configuration</font>\n",
    "**DO NOT CHANGE** the seed from 21. We will not be able to grade your project if that happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SystemConfiguration:\n",
    "    '''\n",
    "    Describes the common system setting needed for reproducible training\n",
    "    '''\n",
    "    seed: int = 21  # seed number to set the state of all random number generators\n",
    "    cudnn_benchmark_enabled: bool = True  # enable CuDNN benchmark for the sake of performance\n",
    "    cudnn_deterministic: bool = True  # make cudnn deterministic (reproducible training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:green\">4.1.2. Training Configuration</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfiguration:\n",
    "    '''\n",
    "    Describes configuration of the training process\n",
    "    '''\n",
    "    batch_size: int = 10 \n",
    "    epochs_count: int = 50  \n",
    "    init_learning_rate: float = 0.1  # initial learning rate for lr scheduler\n",
    "    log_interval: int = 5  \n",
    "    test_interval: int = 1  \n",
    "    data_root: str = \"./cat-dog-panda\" \n",
    "    num_workers: int = 2  \n",
    "    device: str = 'cuda'  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:green\">4.1.3. System Setup</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_system(system_config: SystemConfiguration) -> None:\n",
    "    torch.manual_seed(system_config.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n",
    "        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">4.2. Training Function</font>\n",
    "We are already familiar with the training function. You won't need to make any changes to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_config: TrainingConfiguration, model: nn.Module, optimizer: torch.optim.Optimizer,\n",
    "    train_loader: torch.utils.data.DataLoader, epoch_idx: int\n",
    ") -> None:\n",
    "    \n",
    "    # change model in training mood\n",
    "    model.train()\n",
    "    \n",
    "    # to get batch loss\n",
    "    batch_loss = np.array([])\n",
    "    \n",
    "    # to get batch accuracy\n",
    "    batch_acc = np.array([])\n",
    "        \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        # clone target\n",
    "        indx_target = target.clone()\n",
    "        # send data to device (its is medatory if GPU has to be used)\n",
    "        data = data.to(train_config.device)\n",
    "        # send target to device\n",
    "        target = target.to(train_config.device)\n",
    "\n",
    "        # reset parameters gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass to the model\n",
    "        output = model(data)\n",
    "        \n",
    "        # cross entropy loss\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        \n",
    "        # find gradients w.r.t training parameters\n",
    "        loss.backward()\n",
    "        # Update parameters using gardients\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss = np.append(batch_loss, [loss.item()])\n",
    "        \n",
    "        # Score to probability using softmax\n",
    "        prob = F.softmax(output, dim=1)\n",
    "            \n",
    "        # get the index of the max probability\n",
    "        pred = prob.data.max(dim=1)[1]  \n",
    "                        \n",
    "        # correct prediction\n",
    "        correct = pred.cpu().eq(indx_target).sum()\n",
    "            \n",
    "        # accuracy\n",
    "        acc = float(correct) / float(len(data))\n",
    "        \n",
    "        batch_acc = np.append(batch_acc, [acc])\n",
    "            \n",
    "    epoch_loss = batch_loss.mean()\n",
    "    epoch_acc = batch_acc.mean()\n",
    "    print('Epoch: {} \\nTrain Loss: {:.6f} Acc: {:.4f}'.format(epoch_idx, epoch_loss, epoch_acc))\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">4.3. Validation Function</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(\n",
    "    train_config: TrainingConfiguration,\n",
    "    model: nn.Module,\n",
    "    test_loader: torch.utils.data.DataLoader,\n",
    ") -> float:\n",
    "    # \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    count_corect_predictions = 0\n",
    "    for data, target in test_loader:\n",
    "        indx_target = target.clone()\n",
    "        data = data.to(train_config.device)\n",
    "        \n",
    "        target = target.to(train_config.device)\n",
    "        \n",
    "        output = model(data)\n",
    "        # add loss for each mini batch\n",
    "        test_loss += F.cross_entropy(output, target).item()\n",
    "        \n",
    "        # Score to probability using softmax\n",
    "        prob = F.softmax(output, dim=1)\n",
    "        \n",
    "        # get the index of the max probability\n",
    "        pred = prob.data.max(dim=1)[1] \n",
    "        \n",
    "        # add correct prediction count\n",
    "        count_corect_predictions += pred.cpu().eq(indx_target).sum()\n",
    "\n",
    "    # average over number of mini-batches\n",
    "    test_loss = test_loss / len(test_loader)  \n",
    "    \n",
    "    # average over number of dataset\n",
    "    accuracy = 100. * count_corect_predictions / len(test_loader.dataset)\n",
    "    \n",
    "    print(\n",
    "        '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, count_corect_predictions, len(test_loader.dataset), accuracy\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return test_loss, accuracy/100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">4.4. Save the Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, device, model_dir='models', model_file_name='cat_dog_panda_classifier.pt'):\n",
    "    \n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "\n",
    "    model_path = os.path.join(model_dir, model_file_name)\n",
    "\n",
    "    # make sure you transfer the model to cpu.\n",
    "    if device == 'cuda':\n",
    "        model.to('cpu')\n",
    "\n",
    "    # save the state_dict\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    if device == 'cuda':\n",
    "        model.to('cuda')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">4.5. Load the Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, model_dir='models', model_file_name='cat_dog_panda_classifier.pt'):\n",
    "    model_path = os.path.join(model_dir, model_file_name)\n",
    "\n",
    "    # loading the model and getting model parameters by using load_state_dict\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">4.6. Main Function for Training</font>\n",
    "\n",
    "One thing to note here is that before calling the train function, we are calculating the validation loss and we know that it should be close to `log(num_classes)` and the accuracy should be close to `1/num_classes`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model, optimizer, scheduler=None, system_configuration=SystemConfiguration(), \n",
    "         training_configuration=TrainingConfiguration(), data_augmentation=True):\n",
    "    \n",
    "    # system configuration\n",
    "    setup_system(system_configuration)\n",
    "\n",
    "    # batch size\n",
    "    batch_size_to_set = training_configuration.batch_size\n",
    "    # num_workers\n",
    "    num_workers_to_set = training_configuration.num_workers\n",
    "    # epochs\n",
    "    epoch_num_to_set = training_configuration.epochs_count\n",
    "\n",
    "    # if GPU is available use training config, \n",
    "    # else lowers batch_size, num_workers and epochs count\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        batch_size_to_set = 16\n",
    "        num_workers_to_set = 4\n",
    "\n",
    "    # data loader\n",
    "    train_loader, test_loader = get_data(\n",
    "        batch_size=batch_size_to_set,\n",
    "        data_root=training_configuration.data_root,\n",
    "        num_workers=num_workers_to_set,\n",
    "        data_augmentation=data_augmentation\n",
    "    )\n",
    "    \n",
    "    # Update training configuration\n",
    "    training_configuration = TrainingConfiguration(\n",
    "        device=device,\n",
    "        batch_size=batch_size_to_set,\n",
    "        num_workers=num_workers_to_set\n",
    "    )\n",
    "        \n",
    "    # send model to device (GPU/CPU)\n",
    "    model.to(training_configuration.device)\n",
    "\n",
    "    best_loss = torch.tensor(np.inf)\n",
    "    \n",
    "    # epoch train/test loss\n",
    "    epoch_train_loss = np.array([])\n",
    "    epoch_test_loss = np.array([])\n",
    "    \n",
    "    # epch train/test accuracy\n",
    "    epoch_train_acc = np.array([])\n",
    "    epoch_test_acc = np.array([])\n",
    "    \n",
    "    # trainig time measurement\n",
    "    t_begin = time.time()\n",
    "    for epoch in range(training_configuration.epochs_count):\n",
    "        \n",
    "#         Calculate Initial Test Loss\n",
    "        init_val_loss, init_val_accuracy = validate(training_configuration, model, test_loader)\n",
    "        print(\"Initial Test Loss : {:.6f}, \\nInitial Test Accuracy : {:.3f}%\\n\".format(init_val_loss, init_val_accuracy*100))\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train(training_configuration, model, optimizer, train_loader, epoch)\n",
    "        \n",
    "        epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n",
    "        \n",
    "        epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n",
    "\n",
    "        elapsed_time = time.time() - t_begin\n",
    "        speed_epoch = elapsed_time / (epoch + 1)\n",
    "        speed_batch = speed_epoch / len(train_loader)\n",
    "        eta = speed_epoch * training_configuration.epochs_count - elapsed_time\n",
    "        \n",
    "        print(\n",
    "            \"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(\n",
    "                elapsed_time, speed_epoch, speed_batch, eta\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Validate\n",
    "        if epoch % training_configuration.test_interval == 0:\n",
    "            current_loss, current_accuracy = validate(training_configuration, model, test_loader)\n",
    "            \n",
    "            epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n",
    "        \n",
    "            epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n",
    "            \n",
    "            if current_loss < best_loss:\n",
    "                best_loss = current_loss\n",
    "                print('Model Improved. Saving the Model...\\n')\n",
    "                save_model(model, device=training_configuration.device)\n",
    "        \n",
    "                \n",
    "    print(\"Total time: {:.2f}, Best Loss: {:.3f}\".format(time.time() - t_begin, best_loss))\n",
    "    \n",
    "    return model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">4.7. Plot Loss and Accuracy</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(train_loss, val_loss, train_acc, val_acc, colors, \n",
    "                       loss_legend_loc='upper center', acc_legend_loc='upper left', \n",
    "                       fig_size=(20, 10), sub_plot1=(1, 2, 1), sub_plot2=(1, 2, 2)):\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    plt.subplot(sub_plot1[0], sub_plot1[1], sub_plot1[2])\n",
    "    \n",
    "    for i in range(len(train_loss)):\n",
    "        x_train = range(len(train_loss[i]))\n",
    "        x_val = range(len(val_loss[i]))\n",
    "        \n",
    "        min_train_loss = train_loss[i].min()\n",
    "        \n",
    "        min_val_loss = val_loss[i].min()\n",
    "        \n",
    "        plt.plot(x_train, train_loss[i], linestyle='-', color='tab:{}'.format(colors[i]), \n",
    "                 label=\"TRAIN LOSS ({0:.4})\".format(min_train_loss))\n",
    "        plt.plot(x_val, val_loss[i], linestyle='--' , color='tab:{}'.format(colors[i]), \n",
    "                 label=\"VALID LOSS ({0:.4})\".format(min_val_loss))\n",
    "        \n",
    "    plt.xlabel('epoch no.')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(loc=loss_legend_loc)\n",
    "    plt.title('Training and Validation Loss')\n",
    "        \n",
    "    plt.subplot(sub_plot2[0], sub_plot2[1], sub_plot2[2])\n",
    "    \n",
    "    for i in range(len(train_acc)):\n",
    "        x_train = range(len(train_acc[i]))\n",
    "        x_val = range(len(val_acc[i]))\n",
    "        \n",
    "        max_train_acc = train_acc[i].max() \n",
    "        \n",
    "        max_val_acc = val_acc[i].max() \n",
    "        \n",
    "        plt.plot(x_train, train_acc[i], linestyle='-', color='tab:{}'.format(colors[i]), \n",
    "                 label=\"TRAIN ACC ({0:.4})\".format(max_train_acc))\n",
    "        plt.plot(x_val, val_acc[i], linestyle='--' , color='tab:{}'.format(colors[i]), \n",
    "                 label=\"VALID ACC ({0:.4})\".format(max_val_acc))\n",
    "        \n",
    "    plt.xlabel('epoch no.')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(loc=acc_legend_loc)\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    \n",
    "    fig.savefig('sample_loss_acc_plot.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:red\">4.8. Define Model [20 Points]</font>\n",
    "\n",
    "In the section, you will define the CNN model. You need to keep iterating by training various models by changing:\n",
    "    - Number of layers\n",
    "    - Parameters inside the layers\n",
    "    - different types of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "#     YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:red\">4.9. Training [20 Points]</font>\n",
    "You should experiment with\n",
    "    - Optimizers and learning rate schedulers [You can even get good results without a learning rate shceduler]\n",
    "    - Regularization techniques like Data Augmentation, Dropout, BatchNorm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "print(model)\n",
    "\n",
    "# get optimizer\n",
    "train_config = TrainingConfiguration()\n",
    "\n",
    "### CHANGE HERE ###\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr = train_config.init_learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train and validate\n",
    "model, train_loss, train_acc, val_loss, val_acc = main(model, optimizer, scheduler=None, data_augmentation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">4.10. Loss and Accuracy Plot</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(train_loss=[train_loss], \n",
    "                   val_loss=[val_loss], \n",
    "                   train_acc=[train_acc], \n",
    "                   val_acc=[val_acc], \n",
    "                   colors=['blue'], \n",
    "                   loss_legend_loc='upper center', \n",
    "                   acc_legend_loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">5. Sample Prediction</font>\n",
    "\n",
    "Show some sample predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">5.1. Make Predictions</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model, device, batch_input):\n",
    "    \n",
    "    # send model to cpu/cuda according to your system configuration\n",
    "    model.to(device)\n",
    "    \n",
    "    # it is important to do model.eval() before prediction\n",
    "    model.eval()\n",
    "\n",
    "    data = batch_input.to(device)\n",
    "\n",
    "    output = model(data)\n",
    "\n",
    "    # Score to probability using softmax\n",
    "    prob = F.softmax(output, dim=1)\n",
    "\n",
    "    # get the max probability\n",
    "    pred_prob = prob.data.max(dim=1)[0]\n",
    "    \n",
    "    # get the index of the max probability\n",
    "    pred_index = prob.data.max(dim=1)[1]\n",
    "    \n",
    "    return pred_index.cpu().numpy(), pred_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">5.2. Get predictions on a batch</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_prediction(model, data_root, mean, std):\n",
    "    batch_size = 15\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        num_workers = 8\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        num_workers = 2\n",
    "        \n",
    "        \n",
    "    \n",
    "    # transformed data\n",
    "    test_dataset_trans = datasets.ImageFolder(root=data_root, transform=image_common_transforms(mean, std))\n",
    "    \n",
    "    # original image dataset\n",
    "    test_dataset = datasets.ImageFolder(root=data_root, transform=image_preprocess_transforms())\n",
    "    \n",
    "    data_len = test_dataset.__len__()\n",
    "    \n",
    "    interval = int(data_len/batch_size)\n",
    "    \n",
    "    imgs = []\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for i in range(batch_size):\n",
    "        index = i * interval\n",
    "        trans_input, target = test_dataset_trans.__getitem__(index)\n",
    "        img, _ = test_dataset.__getitem__(index)\n",
    "        \n",
    "        imgs.append(img)\n",
    "        inputs.append(trans_input)\n",
    "        targets.append(target)\n",
    "        \n",
    "    inputs = torch.stack(inputs)\n",
    "        \n",
    "    cls, prob = prediction(model, device, batch_input=inputs)\n",
    "    \n",
    "    plt.style.use('default')\n",
    "    plt.rcParams[\"figure.figsize\"] = (15, 9)\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    \n",
    "    for i, target in enumerate(targets):\n",
    "        plt.subplot(3, 5, i+1)\n",
    "        img = transforms.functional.to_pil_image(imgs[i])\n",
    "        plt.imshow(img)\n",
    "        plt.gca().set_title('P:{0}({1:.2}), T:{2}'.format(test_dataset.classes[cls[i]], \n",
    "                                                     prob[i], \n",
    "                                                     test_dataset.classes[targets[i]]))\n",
    "    fig.savefig('sample_prediction.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">5.3. Load Model and Run Inference</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MyModel()\n",
    "\n",
    "m = load_model(m)\n",
    "\n",
    "train_config = TrainingConfiguration()\n",
    "\n",
    "test_data_path = os.path.join(train_config.data_root, 'validation')\n",
    "\n",
    "train_data_path = os.path.join(train_config.data_root, 'training')\n",
    "\n",
    "mean, std = get_mean_std(train_data_path)\n",
    "\n",
    "\n",
    "get_sample_prediction(m, test_data_path, mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that even though the network is trained on a very small subset if data, the predictions are not that bad. This means that our model is ready to be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:red\">6. Display Confusion Matrix [10 Points]</font>\n",
    "Display the confusion matrix for the above problem. You can refer to the earlier lectures on Performance metrics for reference.\n",
    "\n",
    "The output should something similar to this:\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/02/c3_w5_sample_confusion_matrix.png' width=600>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have completed the assignment, <font style=\"color:red\">upload and submit the 2 notebooks and the models folder</font> on the portal so that we can check and provide feedback."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
